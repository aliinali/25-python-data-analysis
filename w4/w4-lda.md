# LDAä¸»é¢˜æ¨¡å‹
## æ–‡æ¡£é¢„å¤„ç†

> ä¸€èˆ¬æ¥è®²ï¼ŒLDAåœ¨è¯„è®ºç­‰çŸ­æ–‡æœ¬ä¸Šçš„æ•ˆæœå¹¶ä¸ç†æƒ³ï¼Œä¸”å¤šæ•°æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ›ç»™è¯é¢˜èµ‹äºˆæ—¶é—´å«ä¹‰ï¼Œä»¥è®¨è®ºå…¶â€œæ³¢åŠ¨æ€§â€ã€‚å› æ­¤ï¼Œå¾€å¾€å…ˆéœ€è¦æŒ‰æ—¶é—´è¿›è¡Œæ–‡æ¡£çš„ç”Ÿæˆï¼Œæ¯”å¦‚ï¼Œå°†æŸä¸€åº—é“ºçš„è¯„è®ºæŒ‰å¹´è¿›è¡Œåˆå¹¶ï¼Œå³å°†æŸåº—é“ºæŸå¹´å‘å¸ƒçš„æ‰€æœ‰è¯„è®ºè§†ä¸ºä¸€ä¸ªæ–‡æ¡£ã€‚è¯·å®ç°ä¸€ä¸ªæ¨¡å—ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªå‡½æ•°ï¼Œå…¶èƒ½å¤Ÿè¯»å–è¯¥æ•°æ®é›†å¹¶å°†ä¹‹åˆ†åº—é“ºï¼ˆå…±8å®¶åº—é“ºï¼Œå¯æ ¹æ®shopIDè¿›è¡ŒåŒºåˆ†ï¼‰å¤„ç†ä»¥å¤©ï¼ˆæˆ–å…¶ä»–æ—¶é—´å•ä½ï¼‰ä¸ºå•ä½çš„æ–‡æ¡£é›†åˆã€‚

### æ€è·¯
æ–°å»ºpythonæ–‡ä»¶dfcut.py,ä¿å­˜åœ¨ä¸»æ–‡ä»¶åŒä¸€æ–‡ä»¶å¤¹ä¸­ã€‚å®šä¹‰å‡½æ•°id_time_cut(file_path, shopid, TimeMode),è¯»å–æ–‡ä»¶ï¼Œæ ¹æ®åº—é“ºIDå’ŒTimeModeç­›é€‰å­è¡¨ï¼Œåˆå¹¶åŒä¸€æ—¶é—´æ–‡æœ¬ï¼Œè¿”å›ä¸€ä¸ªå­—å…¸ã€‚å…¶ä¸­keyä¸ºæ—¶é—´å€¼ï¼Œvalueä¸ºå­—ç¬¦ä¸²åˆ—è¡¨ã€‚

### ä»£ç 
```Python
import pandas as pd
#import os

def id_time_cut(file_path, shopid, TimeMode) -> dict:
    '''å°†æŸä¸€åº—é“ºçš„è¯„è®ºæŒ‰{TimeMode}è¿›è¡Œåˆå¹¶
    è¿”å›å­—å…¸{æ—¶é—´ï¼šlistæ–‡æ¡£}'''
    
    df = pd.read_csv(file_path, encoding='gbk')
    sub_df = df[df['shopID'] == shopid]
    sub_df = sub_df[['cus_comment', TimeMode]]
    text_at_time_dic = {}
    
    #éå†æ¯è¡Œ
    for index,row in sub_df.iterrows():
        time = row[TimeMode]
        comment = row['cus_comment']
        if pd.isnull(comment):  #å»é™¤ç©ºç™½è¯
            continue
        
        if time not in text_at_time_dic:
            text_at_time_dic[time] = []

        text_at_time_dic[time].append(comment)


    return text_at_time_dic
```

### è¾“å‡º
```Python
['åŒçš®å¥¶ å¥½å‘³ æ¯”ä»ä¿¡ å¤§ç¢— äº‘åé¢ å¥½ å¤§ç¢— æŠµé£Ÿ ä¸è¿‡ å¤ªå¤šäºº é˜¿å§¨ éƒ½ å¾ å¤šç† äºº', 'åŒçš®å¥¶ å¾ˆ å¥½åƒ çš„ ä¸è¿‡ ç¯å¢ƒ å¤ª åµ æ‚ äº† è¦ å’Œ äºº æ‹¼æ¡Œ çš„ è¯´', 'é›™çš®å¥¶ æ˜¯ æˆ‘ çš„ è‡³ æ„›å¿… åƒ  è¿™ è£¹ åƒ¹éŒ¢ å¯ä»¥ é›™çš®å¥¶ åš çš„ æ¯”ä»ä¿¡ å¥½', 'åŒçš®å¥¶ æ¯”ä»ä¿¡  å¥½ å¾ˆå¤š å°±æ˜¯ å¤ªæŒ¤ äº† æœåŠ¡ å°± æœ‰ç‚¹ è·Ÿä¸ä¸Š å’¯ æ€»ä½“ é£Ÿç‰© æ°´ å¹³ ä¸é”™', 'æŒ¤å¾— ä¸€å¡Œç³Šæ¶‚ åªèƒ½ æ‹¼æ¡Œ æœåŠ¡å‘˜ å¤§å–Šå¤§å« è·‘æ¥è·‘å» å®Œå…¨ ä¸§å¤± äº† å“ä½ ç¾é£Ÿ çš„ å…´è‡´ å°è±¡ æ‰“ äº† å¤§å¤§çš„ æŠ˜æ‰£ ä¸»é£Ÿ å“ç§ æ¯” ç”œç‚¹ è¿˜ å¤š è™½ç„¶ ä»·æ ¼ æ›´åŠ  çš„ ä¾¿å®œ åæ°” ä¹Ÿ å¾ˆ å“äº® ä½† åŒçš®å¥¶ å’Œ å…¶ä»– ç”œæ°´ ä» å¤–è¡¨ æ¥è¯´ å°± ä¸åŠ ä»ä¿¡ ä¸€å° æ›´æ˜¯ ç”œ çš„ å‘è…» ç‰¹åˆ« æ˜¯ å¥¶ç³Š ç»å¯¹ è‚¥', 'åç”«è·¯ çš„ åº—  ç”Ÿæ„ çœŸçš„ çˆ†å¥½ å¹³å¿ƒè€Œè®º è¿™é‡Œ çš„ åŒçš®å¥¶ è¿˜ ä¸é”™ æ›¾ç» æ˜¯ å°è±¡ ä¸­ çš„ ç¬¬ä¸€å ç›´åˆ° åœ¨ é¦™æ¸¯ åƒ åˆ°ç›Š é¡ºäºº çš„ å˜´ çœŸå® åˆ  å•Š', 'æœ€ æç¬‘ çš„ æ˜¯ æˆ‘ æŠŠ å•å­ å¼„ ä¸¢ äº† å·®ç‚¹ å°± æŠŠ ç«¯ åˆ° æˆ‘ é¢å‰ çš„ åŒçš®å¥¶ ç»™ ç«¯ å›å» å¯èƒ½ æ˜¯ å…ˆå…¥ä¸ºä¸» å§ è€Œä¸” åˆ° è¿™é‡Œ è¦ çš„ æ˜¯ çº¯ çš„ åŒçš®å¥¶ æ²¡åŠ  çº¢è±† è¡¨é¢ çš„ è£‚ç—• å°± ä¸å¯ é®æ© åœ°éœ² äº† å‡ºæ¥ æ„Ÿè§‰ æ•´ä½“ ä¸Š æ¯”ä»ä¿¡ çš„ å† å†» ä¸€ç‚¹ ä½†æ˜¯ çš® å°± æ²¡æœ‰ åšå‡º å‡¤å‡° å¥¶ç³Š å¥½ ç”œ å•Š å°±æ˜¯ å¥¶ç²‰ å†² çš„ æ„Ÿè§‰ ç¬¬å ç”« çš„ å°åƒ å¤ªå¤š äº† æ„Ÿè§‰ èƒƒå£ å®Œå…¨ è£…ä¸ä¸‹ å‘€ æ¯æ¬¡ æƒ³ åƒ äº‘åé¢ éƒ½ æ²¡ åœ°æ–¹ æ”¾ äº†', 'éå¸¸ å¥½åƒ æ¯æ¬¡ å» å¹¿å· éƒ½  è¦ æƒ³ åŠæ³• å» å°å° å°¤å…¶ æ˜¯ å‡¤å‡° å¥¶ç³Š ä¸€çº§ æ£’', 'ä¹Ÿ æ˜¯ ä¸ª è€å­—å· ç‚¹ äº† æ‹›ç‰Œ çš„ åŒçš®å¥¶ æƒ³ å°å° æ­£å®— çš„ å‘³é“ å¦‚ä½• å¯ èƒ½ æ˜¯ å¤©ç”Ÿ ä¸ å–œæ¬¢ è¿™ç§ å‘³é“ çš„ å§ åƒ äº† å‡ å£ å°± æµªè´¹ äº†', 'æœ€ å–œæ¬¢ è¿™é‡Œ çš„ å§œ åŸ‹å¥¶ å…¶ä»– ä¸»é£Ÿ ä¹Ÿ å¾ˆ ä¸é”™ å¦‚ç‰› ä¸‰æ˜Ÿ ä¸ªäºº è®¤ä¸º æ˜¯ å¹¿å· æœ€ å¥½åƒ çš„ äº‘åé¢ ä¹Ÿ å¾ˆ å¥½ å—é’± å°± æœ‰ å¤§å¤§çš„ è™¾ åœ¨ é‡Œé¢']
```
> ç¿»ä¸åˆ°æœ€å¼€å§‹printå‡ºæ¥çš„ä¸œè¥¿äº†ï¼ˆğŸ‘‰ğŸ»ğŸ‘ˆğŸ»ï¼‰...å­—ç¬¦ä¸²åˆ—è¡¨å¤§æ¦‚é•¿è¿™æ ·

## æ–‡æœ¬çš„ç‰¹å¾è¡¨ç¤º
> å®ç°ä¸€ä¸ªæ¨¡å—ï¼Œé€šè¿‡ä¸€ä¸ªæˆ–å¤šä¸ªå‡½æ•°ï¼Œå°†æ¯ä¸ªæ–‡æ¡£è½¬å˜ä¸ºè¯é¢‘ç‰¹å¾è¡¨ç¤ºï¼Œä»¥å½¢æˆæ–‡æ¡£-è¯è¯­çš„è¯é¢‘çŸ©é˜µï¼Œå¯ä»¥é€‰æ‹©ä½¿ç”¨sklearnä¸­çš„CountVectorizerå’ŒTfidfVectorizerä¸¤ç§æ–¹å¼ã€‚ä¹Ÿå¯ä»¥ä½¿ç”¨gensimä¸­çš„dictionary.doc2bowç­‰ã€‚

### æ€è·¯
è¿™é‡Œä½¿ç”¨äº†sklearnåº“çš„CountVectorizerã€‚æ–°å»ºDocToBow.pyæ–‡ä»¶ï¼Œä¿å­˜åœ¨ä¸»æ–‡ä»¶åŒä¸€æ–‡ä»¶å¤¹ä¸‹ã€‚å®šä¹‰Doc2Bowå‡½æ•°,è¾“å…¥id_time_cutè¿”å›çš„{æ—¶é—´ï¼šå­—ç¬¦ä¸²åˆ—è¡¨}å­—å…¸ï¼Œè¿”å›{æ—¶é—´ï¼šçŸ©é˜µ}å­—å…¸

### ä»£ç 
 ```Python
from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd

def Doc2Bow(text_at_time_dic) -> dict:
    '''return {time:è¯é¢‘çŸ©é˜µ} '''
    matrix_at_time_dic = {}
    for time, doc in text_at_time_dic.items():
        vectorizer = CountVectorizer()
        matrix = vectorizer.fit_transform(doc)
        matrix_at_time_dic[time] = matrix
    return matrix_at_time_dic
```
### è¾“å‡º
```python
{2018: <Compressed Sparse Row sparse matrix of dtype 'int64'
        with 90191 stored elements and shape (2624, 11936)>, 2017: <Compressed Sparse Row sparse matrix of dtype 'int64'
        with 69990 stored elements and shape (2108, 10416)>, 2016: <Compressed Sparse Row sparse matrix of dtype 'int64'
        with 49613 stored elements and shape (1737, 8563)
```
å¯ä»¥çœ‹è§2018å¯¹åº”çš„çŸ©é˜µæ˜¯æœ€å¤§çš„ï¼Œè¯´æ˜2018çš„æ–‡æ¡£å…·æœ‰æœ€ä¸°å¯Œçš„è¯æ±‡è¡¨ï¼Œå½“ç„¶ï¼Œä¹Ÿæœ‰å¯èƒ½æ˜¯åœç”¨è¯è¿‡æ»¤ä¸å……åˆ†

## 3.æ–‡æœ¬çš„è¯é¢˜åˆ†æ
> å®ç°ä¸€ä¸ªæ¨¡å—ï¼Œé€šè¿‡ä¸€ä¸ªæˆ–å¤šä¸ªå‡½æ•°ï¼Œå€ŸåŠ©sklearn.decompositionä¸­çš„LatentDirichletAllocationæ„å»ºä¸»é¢˜æ¨¡å‹ï¼ˆè¯é¢˜æ•°ç›®å¯ä»¥è‡ªä¸»æŒ‡å®šï¼‰ï¼Œå¹¶å¯¹å‘ç°çš„ä¸»é¢˜è¿›è¡Œåˆ†æï¼ˆæ¯ä¸ªä¸»é¢˜å¯¹åº”çš„è¯è¯­å¯åˆ©ç”¨model.components_æ¥æŸ¥çœ‹ï¼Œæ¯ç¯‡æ–‡æ¡£çš„ä¸»é¢˜æ¦‚ç‡åˆ†å¸ƒå¯é€šè¿‡model.transformæ¥æŸ¥çœ‹ï¼‰ã€‚ä¹Ÿå¯ä»¥å‚è€ƒdemoé‡Œçš„ldav.pyï¼Œç”¨gensimè¿›è¡ŒLDAåˆ†æï¼Œå¹¶è¿›è¡Œå¯è§†åŒ–.

### æ€è·¯
æ–°å»ºpythonæ–‡ä»¶find_feature.py,ä¿å­˜åœ¨ä¸»æ–‡ä»¶åŒä¸€æ–‡ä»¶å¤¹ä¸­ã€‚å®šä¹‰å‡½æ•°lda_analysis(doc,n_topics,n_words),å°†æ–‡æ¡£è½¬æ¢ä¸ºè¯é¢‘çŸ©é˜µï¼Œè·å–ä¸»é¢˜å…³é”®è¯åŠæ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒ

### ä»£ç 
```python

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

def lda_analysis(documents, n_topics=5, n_words=10):

    # å°†æ–‡æ¡£è½¬æ¢ä¸ºè¯é¢‘çŸ©é˜µ
    vectorizer = CountVectorizer(stop_words='english')
    X = vectorizer.fit_transform(documents)
    
    # æ„å»º LDA æ¨¡å‹
    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)
    lda.fit(X)
    
    # è·å–ä¸»é¢˜å…³é”®è¯
    feature_names = vectorizer.get_feature_names_out()
    topic_keywords = {}
    for topic_idx, topic in enumerate(lda.components_):
        top_features_ind = topic.argsort()[:-n_words - 1:-1]
        topic_keywords[topic_idx] = [feature_names[i] for i in top_features_ind]
    
    # è·å–æ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒ
    doc_topic_distr = lda.transform(X)
    
    return topic_keywords, doc_topic_distr
```
ä¸»æ–‡ä»¶ä¸­ï¼šå…ˆå°†æ‰€æœ‰å­—ç¬¦ä¸²åˆå¹¶ï¼Œå¯¹æ‰€æœ‰æ–‡æ¡£ä¾æ¬¡è°ƒç”¨å‡½æ•°ï¼Œæ‰“å°ä¸»é¢˜å…³é”®è¯åŠæ–‡æ¡£ä¸»é¢˜åˆ†å¸ƒ
```python

combined_documents = {time: " ".join(doc) for time, doc in doc_at_time_dic.items()}
documents = list(combined_documents.values())
times = list(combined_documents.keys())

# å¯¹æ‰€æœ‰æ–‡æ¡£è¿›è¡Œ LDA åˆ†æ
topic_keywords, doc_topic_distr = lda_analysis(documents, n_topics=k, n_words=5)

# æ‰“å°æ¯ä¸ªæ—¶é—´ç‚¹çš„ä¸»é¢˜å…³é”®è¯å’Œæ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒ
for time, doc_topic in zip(times, doc_topic_distr):
    print(f"æ—¶é—´: {time}")
    print("ä¸»é¢˜å…³é”®è¯:")
    for topic_idx, keywords in topic_keywords.items():
        print(f"  ä¸»é¢˜ {topic_idx}: {' '.join(keywords)}")
    print("æ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒ:")
    print(f"  ä¸»é¢˜åˆ†å¸ƒ: {doc_topic}")
    print("-" * 50)
```
### è¾“å‡ºï¼ˆéƒ¨åˆ†ï¼‰
```python 
æ—¶é—´: 2013
ä¸»é¢˜å…³é”®è¯:
  ä¸»é¢˜ 0: åŒçš®å¥¶ ä¸‰æ˜Ÿ ä¸è¿‡ å‘³é“ å¥½åƒ
  ä¸»é¢˜ 1: æŒ¤å¾— ä¸€å¡Œç³Šæ¶‚ å…´è‡´ å“äº® å¤©ç”Ÿ
  ä¸»é¢˜ 2: é¡ºäºº åœ°éœ² æ„›å¿… åšå‡º åˆ°ç›Š
  ä¸»é¢˜ 3: åŒçš®å¥¶ å¥½åƒ å‘³é“ ç”œå“ å¹¿å·
æ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒ:
  ä¸»é¢˜åˆ†å¸ƒ: [5.79731315e-01 9.88632683e-06 9.88637359e-06 4.20248912e-01]
--------------------------------------------------       
æ—¶é—´: 2012
ä¸»é¢˜å…³é”®è¯:
  ä¸»é¢˜ 0: åŒçš®å¥¶ ä¸‰æ˜Ÿ ä¸è¿‡ å‘³é“ å¥½åƒ
  ä¸»é¢˜ 1: æŒ¤å¾— ä¸€å¡Œç³Šæ¶‚ å…´è‡´ å“äº® å¤©ç”Ÿ
  ä¸»é¢˜ 2: é¡ºäºº åœ°éœ² æ„›å¿… åšå‡º åˆ°ç›Š
  ä¸»é¢˜ 3: åŒçš®å¥¶ å¥½åƒ å‘³é“ ç”œå“ å¹¿å·
æ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒ:
  ä¸»é¢˜åˆ†å¸ƒ: [6.24590158e-01 1.80292163e-05 1.80291220e-05 3.75373784e-01]
--------------------------------------------------       
æ—¶é—´: 2011
ä¸»é¢˜å…³é”®è¯:
  ä¸»é¢˜ 0: åŒçš®å¥¶ ä¸‰æ˜Ÿ ä¸è¿‡ å‘³é“ å¥½åƒ
  ä¸»é¢˜ 1: æŒ¤å¾— ä¸€å¡Œç³Šæ¶‚ å…´è‡´ å“äº® å¤©ç”Ÿ
  ä¸»é¢˜ 2: é¡ºäºº åœ°éœ² æ„›å¿… åšå‡º åˆ°ç›Š
  ä¸»é¢˜ 3: åŒçš®å¥¶ å¥½åƒ å‘³é“ ç”œå“ å¹¿å·
æ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒ:
  ä¸»é¢˜åˆ†å¸ƒ: [7.55236675e-01 1.55212727e-05 1.55211980e-05 2.44732283e-01]
--------------------------------------------------       
æ—¶é—´: 2010
ä¸»é¢˜å…³é”®è¯:
  ä¸»é¢˜ 0: åŒçš®å¥¶ ä¸‰æ˜Ÿ ä¸è¿‡ å‘³é“ å¥½åƒ
  ä¸»é¢˜ 1: æŒ¤å¾— ä¸€å¡Œç³Šæ¶‚ å…´è‡´ å“äº® å¤©ç”Ÿ
  ä¸»é¢˜ 2: é¡ºäºº åœ°éœ² æ„›å¿… åšå‡º åˆ°ç›Š
  ä¸»é¢˜ 3: åŒçš®å¥¶ å¥½åƒ å‘³é“ ç”œå“ å¹¿å·
æ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒ:
  ä¸»é¢˜åˆ†å¸ƒ: [8.70977957e-01 1.79368238e-05 1.79367507e-05 1.28986169e-01]
--------------------------------------------------       
æ—¶é—´: 2009
ä¸»é¢˜å…³é”®è¯:
  ä¸»é¢˜ 0: åŒçš®å¥¶ ä¸‰æ˜Ÿ ä¸è¿‡ å‘³é“ å¥½åƒ
  ä¸»é¢˜ 1: æŒ¤å¾— ä¸€å¡Œç³Šæ¶‚ å…´è‡´ å“äº® å¤©ç”Ÿ
  ä¸»é¢˜ 2: é¡ºäºº åœ°éœ² æ„›å¿… åšå‡º åˆ°ç›Š
  ä¸»é¢˜ 3: åŒçš®å¥¶ å¥½åƒ å‘³é“ ç”œå“ å¹¿å·
æ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒ:
  ä¸»é¢˜åˆ†å¸ƒ: [9.83038081e-01 1.71415323e-05 1.71414817e-05 1.69276364e-02]
--------------------------------------------------       
æ—¶é—´: 2008
ä¸»é¢˜å…³é”®è¯:
  ä¸»é¢˜ 0: åŒçš®å¥¶ ä¸‰æ˜Ÿ ä¸è¿‡ å‘³é“ å¥½åƒ
  ä¸»é¢˜ 1: æŒ¤å¾— ä¸€å¡Œç³Šæ¶‚ å…´è‡´ å“äº® å¤©ç”Ÿ
  ä¸»é¢˜ 2: é¡ºäºº åœ°éœ² æ„›å¿… åšå‡º åˆ°ç›Š
  ä¸»é¢˜ 3: åŒçš®å¥¶ å¥½åƒ å‘³é“ ç”œå“ å¹¿å·
æ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒ:
  ä¸»é¢˜åˆ†å¸ƒ: [9.99976193e-01 7.76579575e-06 7.76577635e-06 8.27570244e-06]
--------------------------------------------------       
æ—¶é—´: 2007
ä¸»é¢˜å…³é”®è¯:
  ä¸»é¢˜ 0: åŒçš®å¥¶ ä¸‰æ˜Ÿ ä¸è¿‡ å‘³é“ å¥½åƒ
  ä¸»é¢˜ 1: æŒ¤å¾— ä¸€å¡Œç³Šæ¶‚ å…´è‡´ å“äº® å¤©ç”Ÿ
  ä¸»é¢˜ 2: é¡ºäºº åœ°éœ² æ„›å¿… åšå‡º åˆ°ç›Š
  ä¸»é¢˜ 3: åŒçš®å¥¶ å¥½åƒ å‘³é“ ç”œå“ å¹¿å·
æ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒ:
  ä¸»é¢˜åˆ†å¸ƒ: [9.99935843e-01 2.08460070e-05 2.08459379e-05 2.24649881e-05]
--------------------------------------------------       
æ—¶é—´: 2006
ä¸»é¢˜å…³é”®è¯:
  ä¸»é¢˜ 0: åŒçš®å¥¶ ä¸‰æ˜Ÿ ä¸è¿‡ å‘³é“ å¥½åƒ
  ä¸»é¢˜ 1: æŒ¤å¾— ä¸€å¡Œç³Šæ¶‚ å…´è‡´ å“äº® å¤©ç”Ÿ
  ä¸»é¢˜ 2: é¡ºäºº åœ°éœ² æ„›å¿… åšå‡º åˆ°ç›Š
  ä¸»é¢˜ 3: åŒçš®å¥¶ å¥½åƒ å‘³é“ ç”œå“ å¹¿å·
æ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒ:
  ä¸»é¢˜åˆ†å¸ƒ: [9.99701541e-01 9.72162674e-05 9.72176553e-05 1.04025501e-04]
--------------------------------------------------
```
## 4.åºåˆ—åŒ–ä¿å­˜
> åˆ©ç”¨pickleæˆ–jsonå¯¹æ‰€å¾—åˆ°çš„ldaæ¨¡å‹ã€å¯¹åº”çš„è¯é¢‘çŸ©é˜µã€ä»¥åŠç‰¹å¾è¡¨ç¤ºç­‰è¿›è¡Œåºåˆ—åŒ–ä¿å­˜ã€‚


## 5.æ ¹æ®å›°æƒ‘åº¦é€‰å–æœ€ä¼˜è¯é¢˜æ•°
> è¶…å‚æ•°kï¼ˆå³è¯é¢˜çš„æ•°ç›®ï¼‰å˜åŒ–æ—¶ï¼Œè¯„ä»·LDAæ¨¡å‹çš„ä¸€ä¸ªæŒ‡æ ‡å³å›°æƒ‘åº¦ï¼ˆlda.perplexityï¼‰ä¼šéšä¹‹æ³¢åŠ¨ï¼Œå°è¯•ç»˜åˆ¶å›°æƒ‘åº¦éšè¯é¢˜æ•°ç›®kå˜åŒ–çš„æ›²çº¿ï¼Œæ‰¾åˆ°è¾ƒä¼˜çš„kã€‚

### æ€è·¯
å…ˆä»doc_at_time_dicä¸­æŠŠæ‰€æœ‰å­—ç¬¦ä¸²åˆ—è¡¨æå–å‡ºæ¥ï¼Œåˆå¹¶ä¸ºdocsï¼Œç»˜åˆ¶å›°æƒ‘åº¦éšè¯é¢˜æ•°ç›®å˜åŒ–çš„æ›²çº¿ï¼Œç†æƒ³çš„kå€¼åº”è¯¥æ˜¯ï¼šåœ¨kå‰æ›²çº¿é€æ¸ä¸‹é™ï¼Œkåè¶‹äºå¹³ç¨³

### ä»£ç 
```python
#ä¸ºæ‰¾åˆ°å›°æƒ‘åº¦ å…ˆæŠŠæ‰€æœ‰docæå–å‡ºæ¥
docs = []
for time,doc in doc_at_time_dic.items():
    docs += doc


vectorizer = CountVectorizer()
X = vectorizer.fit_transform(docs)

perplexity_scores = []
k_range = range(5,51,5) # kçš„èŒƒå›´
for k in k_range:
    lda = LatentDirichletAllocation(n_components=k,max_iter= 2000)
    lda.fit(X)
    perplexity_scores.append(lda.perplexity(X))
plt.plot(k_range, perplexity_scores, '-o')
plt.xlabel('Number of topics')
plt.ylabel('Perplexity')
plt.show()
```
### è¾“å‡º
è¿™ä¸€æ­¥åšäº†å‡ æ¬¡è°ƒå‚æœ€åç»“æœè¿˜æ˜¯ä¸å¥½çœ‹...æˆªæ­¢æˆ‘å†™æŠ¥å‘Šçš„æ—¶é—´ï¼ˆ2025å¹´3æœˆ24æ—¥14:45:14ï¼‰ï¼Œä¾æ—§æ²¡æœ‰è·‘å‡ºå¥½çš„ç»“æœï¼Œå…ˆæŠŠk_range = range(11), max_iter = 1000çš„ç»“æœæ”¾å‡ºæ¥å ä¸ªå‘......å¸Œæœ›åŠ©æ•™æ£€æŸ¥ä½œä¸šçš„æ—¶å€™æˆ‘å·²ç»è·‘å‡ºæ¥äº†ğŸ¥²ğŸ¥²ğŸ¥²
